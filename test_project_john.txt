AI Resume Evaluation System – Project Report

Background:
Recruitment teams often struggle with screening large volumes of CVs and assessing candidate projects consistently. Manual evaluation is time-consuming and prone to bias.

Solution:
I developed an AI-driven resume and project evaluation system to automate candidate screening. The system uses FastAPI for backend APIs, Celery + Redis for asynchronous tasks, and PostgreSQL for data persistence. LLMs (OpenAI GPT-4 + Mistral) are integrated for CV analysis and project scoring, with ChromaDB used for RAG-based retrieval of job descriptions and evaluation rubrics.

Implementation:
- CV extraction pipeline using LLM to parse structured data (name, skills, experience, projects).
- Job description and rubric stored in vector DB (ChromaDB) for retrieval.
- Evaluation workflow:
  1. Compare candidate CV with job description → produce match rate + feedback.
  2. Evaluate project report based on rubric (correctness, code quality, resilience, documentation, creativity).
  3. Aggregate into final scoring.
- Backend built with FastAPI, SQLModel, Alembic, and psycopg2. Logging with Loguru, retries on API calls to handle rate limits.

Results & Evaluation:
- Correctness: System meets all main requirements (resume parsing, CV-job matching, project scoring).
- Code Quality: Modular design with clear separation (api, models, tasks). Linting + type hints applied.
- Resilience: Implemented retry/backoff for LLM API calls, Celery for long-running tasks, and error logging.
- Documentation: README explains setup, architecture, and evaluation flow. Inline comments added in critical functions.
- Creativity: Added real-time status tracking and support for multiple file formats (PDF, DOCX, TXT).

Limitations:
- Model accuracy depends on LLM performance; further fine-tuning could improve results.
- No frontend filtering or analytics dashboard yet (MVP focus).
- Scaling not yet tested under very high concurrency.

Conclusion:
The project demonstrates an end-to-end pipeline for automated candidate evaluation. It balances correctness and clarity while leaving room for future improvements such as model fine-tuning and dashboard features.
